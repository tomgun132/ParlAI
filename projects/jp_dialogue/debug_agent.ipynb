{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from parlai.core.params import ParlaiParser\n",
    "from parlai.core.dict import DictionaryAgent\n",
    "from parlai.scripts.train_model import setup_args as tm_setupargs\n",
    "from parlai.scripts.train_model import TrainLoop\n",
    "from parlai.core.agents import create_agent\n",
    "from parlai.core.worlds import create_task\n",
    "from itertools import islice\n",
    "from ja_sentpiece_tokenizer import FullTokenizer\n",
    "import json\n",
    "import time\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "SHARED = {}\n",
    "def setup_interactive(shared):\n",
    "    parser = tm_setupargs()\n",
    "    parser.set_params(batchsize=5, fixed_candidates_path='/installation/~/ParlAI/data/rachel/topical_cands.json')\n",
    "    SHARED['opt'] = parser.parse_args([], print_args=True)\n",
    "\n",
    "#    SHARED['opt']['task'] = 'projects.jp_dialogue.tasks.agents'\n",
    "    SHARED['opt']['task'] = 'parlai.agents.local_human.local_human:LocalHumanAgent'\n",
    "    SHARED['opt']['model'] = 'projects.jp_dialogue.jp_retrieval.retrieval_agents:BertJPRanker'\n",
    "#     SHARED['opt']['dict_class'] = 'projects.jp_dialogue.jp_retrieval.retrieval_agents:UniBiDictionaryAgent'\n",
    "#     SHARED['opt']['dict_build_first'] = True\n",
    "#     SHARED['opt']['dict_minfreq'] = 5\n",
    "    SHARED['opt']['model_file'] = '/installation/~/ParlAI/data/models/rachel/bibert_poly_ranker'\n",
    "    SHARED['opt']['no_cuda'] = True\n",
    "    SHARED['opt']['history_size'] = 1\n",
    "    SHARED['opt']['truncate'] = -1\n",
    "    SHARED['opt']['interactive_mode'] = True\n",
    "    SHARED['opt']['candidates'] = 'batch'\n",
    "    SHARED['opt']['eval_candidates'] = 'fixed'\n",
    "    SHARED['opt']['encode_candidate_vecs'] = True\n",
    "    SHARED['opt']['fixed_candidates_path'] = '/installation/~/ParlAI/data/rachel/topical_cands.json'\n",
    "    SHARED['opt']['fixed_candidate_vecs'] = 'fixed'\n",
    "#     SHARED['opt']['scoring_func'] = 'scaled'\n",
    "#     SHARED['opt']['embedding_size'] = 300\n",
    "#     SHARED['opt']['h_layer_num'] = 3\n",
    "#     SHARED['opt']['h_dim'] = 512\n",
    "#     SHARED['opt']['h_act_func'] = 'swish'\n",
    "#     SHARED['opt']['linear_dim'] = 1024\n",
    "#     SHARED['opt']['n_heads'] = 12\n",
    "#     SHARED['opt']['n_layers'] = 1\n",
    "#     SHARED['opt']['ffn_size'] = 3072\n",
    "#     SHARED['opt']['dropout'] = 0.1\n",
    "#     SHARED['opt']['attention_dropout'] = 0.1\n",
    "#     SHARED['opt']['relu_dropout'] = 0.0\n",
    "#     SHARED['opt']['learn_positional_embeddings'] = True\n",
    "#     SHARED['opt']['embeddings_scale'] = False\n",
    "#     SHARED['opt']['activation'] = 'gelu'\n",
    "#     SHARED['opt']['variant'] = 'xlm'\n",
    "#     SHARED['opt']['output_scaling'] = 1.0\n",
    "#     SHARED['opt']['eval_candidates'] = ''\n",
    "#     SHARED['opt']['learningrate'] = 5e-05\n",
    "#     SHARED['opt']['momentum'] = 0\n",
    "#     SHARED['opt']['optimizer'] = 'sgd'\n",
    "    SHARED['opt']['lr_scheduler'] ='reduceonplateau'\n",
    "    SHARED['opt']['out_dim'] =768\n",
    "    SHARED['opt']['add_transformer_layer']=False\n",
    "    SHARED['opt']['pull_from_layer']=-1\n",
    "    SHARED['opt']['bert_aggregation']='mean'\n",
    "    SHARED['opt']['dict_maxexs']=0\n",
    "    SHARED['opt']['train_folder']='rachel'\n",
    "    # Create model and assign it to the specified task\n",
    "    SHARED['tokenizer'] = FullTokenizer(SHARED['opt']['datapath'] + '/models/')\n",
    "    SHARED['agent'] = create_agent(SHARED.get('opt'), requireModelExists=True)\n",
    "    SHARED['world'] = create_task(SHARED.get('opt'), [SHARED['agent']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_responses(agent, text, topic):\n",
    "    # topic = classify(text)\n",
    "    obs = {\"text\": text, 'episode_done': False, 'topic': topic}\n",
    "    agent.observe(obs)\n",
    "    batch = agent.batchify([agent.observation])\n",
    "    batchsize = (\n",
    "        batch.text_vec.size(0)\n",
    "        if batch.text_vec is not None\n",
    "        else batch.image.size(0)\n",
    "    )\n",
    "    agent.model.eval()\n",
    "    cands = agent.fixed_candidates[topic]\n",
    "    cand_vecs = agent.fixed_candidate_vecs[topic]\n",
    "    cand_encs = agent.fixed_candidate_encs[topic]\n",
    "    scores = agent.score_candidates(batch, cand_vecs, cand_encs=cand_encs)\n",
    "\n",
    "    if agent.rank_top_k > 0:\n",
    "        sorted_scores, ranks = scores.topk(\n",
    "            min(agent.rank_top_k, scores.size(1)), 1, largest=True\n",
    "        )\n",
    "    else:\n",
    "        sorted_scores, ranks = scores.sort(1, descending=True)\n",
    "\n",
    "    # sorted_scores = F.softmax(sorted_scores, dim=-1)\n",
    "\n",
    "    ranks = ranks.cpu()\n",
    "    max_preds = agent.opt['cap_num_predictions']\n",
    "    cand_preds = []\n",
    "    for i, ordering in enumerate(ranks):\n",
    "        if cand_vecs.dim() == 2:\n",
    "            cand_list = cands\n",
    "        elif cand_vecs.dim() == 3:\n",
    "            cand_list = cands[i]\n",
    "        # using a generator instead of a list comprehension allows\n",
    "        # to cap the number of elements.\n",
    "        cand_preds_generator = (\n",
    "            cand_list[rank] for rank in ordering if rank < len(cand_list)\n",
    "        )\n",
    "        cand_preds.append(list(islice(cand_preds_generator, max_preds)))\n",
    "\n",
    "    if (\n",
    "        agent.opt.get('repeat_blocking_heuristic', True)\n",
    "        and agent.eval_candidates == 'fixed'\n",
    "    ):\n",
    "        cand_preds = agent.block_repeats(cand_preds)\n",
    "\n",
    "    preds = [cand_preds[i][0] for i in range(batchsize)]\n",
    "    return preds, cand_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ Main ParlAI Arguments: ] \n",
      "[  batchsize: 5 ]\n",
      "[  datapath: d:\\installation\\~\\parlai\\data ]\n",
      "[  datatype: train ]\n",
      "[  download_path: d:\\installation\\~\\parlai\\downloads ]\n",
      "[  hide_labels: False ]\n",
      "[  image_mode: raw ]\n",
      "[  init_opt: None ]\n",
      "[  multitask_weights: [1] ]\n",
      "[  numthreads: 1 ]\n",
      "[  show_advanced_args: False ]\n",
      "[  task: None ]\n",
      "[ ParlAI Model Arguments: ] \n",
      "[  dict_class: None ]\n",
      "[  init_model: None ]\n",
      "[  model: None ]\n",
      "[  model_file: None ]\n",
      "[ Training Loop Arguments: ] \n",
      "[  aggregate_micro: False ]\n",
      "[  display_examples: False ]\n",
      "[  eval_batchsize: None ]\n",
      "[  evaltask: None ]\n",
      "[  load_from_checkpoint: False ]\n",
      "[  max_train_time: -1 ]\n",
      "[  metrics: default ]\n",
      "[  num_epochs: -1 ]\n",
      "[  save_after_valid: False ]\n",
      "[  save_every_n_secs: -1 ]\n",
      "[  short_final_eval: False ]\n",
      "[  validation_cutoff: 1.0 ]\n",
      "[  validation_every_n_epochs: -1 ]\n",
      "[  validation_every_n_secs: -1 ]\n",
      "[  validation_max_exs: -1 ]\n",
      "[  validation_metric: accuracy ]\n",
      "[  validation_metric_mode: None ]\n",
      "[  validation_patience: 10 ]\n",
      "[  validation_share_agent: False ]\n",
      "[ Tensorboard Arguments: ] \n",
      "[  tensorboard_log: False ]\n",
      "[ PytorchData Arguments: ] \n",
      "[  batch_length_range: 5 ]\n",
      "[  batch_sort_cache_type: pop ]\n",
      "[  batch_sort_field: text ]\n",
      "[  numworkers: 4 ]\n",
      "[  pytorch_context_length: -1 ]\n",
      "[  pytorch_datapath: None ]\n",
      "[  pytorch_include_labels: True ]\n",
      "[  pytorch_preprocess: False ]\n",
      "[  pytorch_teacher_batch_sort: False ]\n",
      "[  pytorch_teacher_dataset: None ]\n",
      "[  pytorch_teacher_task: None ]\n",
      "[  shuffle: False ]\n",
      "[ Dictionary Loop Arguments: ] \n",
      "[  dict_include_test: False ]\n",
      "[  dict_include_valid: False ]\n",
      "[  dict_maxexs: -1 ]\n",
      "[  log_every_n_secs: 2 ]\n",
      "[ Dictionary Arguments: ] \n",
      "[  bpe_debug: False ]\n",
      "[  dict_endtoken: __end__ ]\n",
      "[  dict_file: None ]\n",
      "[  dict_initpath: None ]\n",
      "[  dict_language: english ]\n",
      "[  dict_lower: False ]\n",
      "[  dict_max_ngram_size: -1 ]\n",
      "[  dict_maxtokens: -1 ]\n",
      "[  dict_minfreq: 0 ]\n",
      "[  dict_nulltoken: __null__ ]\n",
      "[  dict_starttoken: __start__ ]\n",
      "[  dict_textfields: text,labels ]\n",
      "[  dict_tokenizer: re ]\n",
      "[  dict_unktoken: __unk__ ]\n",
      "[ ParlAI Image Preprocessing Arguments: ] \n",
      "[  image_cropsize: 224 ]\n",
      "[  image_size: 256 ]\n",
      "[ Current ParlAI commit: 742c645321b1f171c8103bd3d0651ef904114a45 ]\n",
      "Loaded a trained SentencePiece model.\n",
      "[ warning: overriding opt['batchsize'] to 5 (previously: 16 )]\n",
      "[ warning: overriding opt['fixed_candidates_path'] to /installation/~/ParlAI/data/rachel/topical_cands.json (previously: /home/ubuntu/workspace/ParlAI/data/rachel/tokenized_cands.txt )]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\installation\\~\\parlai\\parlai\\agents\\transformer\\modules.py:32: UserWarning: Installing APEX can give a significant speed boost.\n",
      "  warn_once(\"Installing APEX can give a significant speed boost.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ Using CUDA ]\n",
      "Dictionary: loading dictionary from /installation/~/ParlAI/data/models/rachel/bibert_poly_ranker.dict\n",
      "[ num words =  9 ]\n",
      "[BertJPRanker: full interactive mode on.]\n",
      "Total parameters: 222415872\n",
      "Trainable parameters:  222415872\n",
      "Loading existing model parameters from /installation/~/ParlAI/data/models/rachel/bibert_poly_ranker\n",
      "[ Loading fixed candidate set from /installation/~/ParlAI/data/rachel/topical_cands.json ]\n",
      "[ Loading fixed candidate set vectors from /installation/~/ParlAI/data/models/rachel\\bibert_poly_ranker.topical_cands.vecs ]\n",
      "[ Loading fixed candidate set encodings from /installation/~/ParlAI/data/models/rachel\\bibert_poly_ranker.topical_cands.encs ]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tomgun\\Anaconda3\\lib\\site-packages\\torch\\nn\\_reduction.py:43: UserWarning: size_average and reduce args will be deprecated, please use reduction='mean' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[creating task(s): parlai.agents.local_human.local_human:LocalHumanAgent]\n",
      "Enter [DONE] if you want to end the episode.\n",
      "\n",
      "Enter [DONE] if you want to end the episode.\n",
      "\n",
      "Enter [DONE] if you want to end the episode.\n",
      "\n",
      "Enter [DONE] if you want to end the episode.\n",
      "\n",
      "Enter [DONE] if you want to end the episode.\n",
      "\n",
      "Enter [DONE] if you want to end the episode.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "setup_interactive(SHARED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['▁ 夢 について ですか 。 いい です ね 。 あなた の 夢 が 知り たい です 。']\n",
      "[['▁ 夢 について ですか 。 いい です ね 。 あなた の 夢 が 知り たい です 。', '▁ は い 。 宇宙 に行く の が 私の 夢 です 。', '▁今 は お金 がある 人 しか 行 け ません が 、 後 何 年 か で 自由に 宇宙 に 行 ける 日 が 来 そう です 。', '▁ は い 。 宇宙 に行く ことは 私の 夢 です 。', '▁ は い 。 とにかく どんな 世界 なのか 見て みたい です 。', '▁ いい です ね 。 そういう 話 好き です 。', '▁それは すごい です ね 。', '▁ そう ですか 。 私の 夢 は 宇宙 に行く ことで す 。', '▁ 解明 されて ないこと が 、 たくさん あって お もし ろう そう じゃない ですか 。', '▁ は い 。 宇宙 に行く ことが 私の 夢 です 。', '▁ そう です ね 。 お金 があれば もう 行 け ます し 。', '▁それは 行って みて から 考え ます 。 とにかく 行って みたい です ね 。', '▁ あ ー 。 私 も 宇宙 は大 好き です 。', '▁ すごい 。 か っこ いい です ね 。', '▁ そう ですか 。 私は 新しい もの を見て みたい です 。', '▁ え 。 言う の 恥 ず か しい です 。 先に あなた の 夢 を教え て ください 。', '▁韓国 が お 好きな んです ね 。', '▁ 分かり ました 。 私の 夢 はず ば り 宇宙 に行く ことで す 。', '▁ ふ ふ ふ 。 私は 宇宙 好きな んです 。', '▁ 宇宙 から 地球 を見て みたい です 。', '▁ 言語 がある のか も 謎 です ね 。', '▁それは ずる い です ね 。', '▁ は い 、 聞き たい です 。', '▁ そうな んです か 。 自分 探し 中 です ね 。', '▁ ない です ね 。 いつか 行って みたい です 。', '▁ すごい です ね 。 山 登り が 好きな んです ね 。', '▁ すごい です ね 。 その 話 もっと 聞かせ て ください 。', '▁ は い 。 一番 行って みたい ところで す 。', '▁ 詳しい かどうか は 分かり ません が 、 時々 調べ たり します よ 。 ブラックホール について とか 。', '▁ 普通 に カレー とか ラーメン が 食べられる みたい です 。', '▁ 私の 夢 は 宇宙 に行く ことで す 。', '▁ 数え るほど しか 行って ない です よ 。', '▁それは あなた 次第 です ね 。', '▁ 火星 に興味 が あります 。 space x が 大 好き で 。', '▁それは 怖 かった です ね 。', '▁他の 人 が行った ことがない 場所に 行って みたい から です 。', '▁ なん か 楽 し そう です ね 。', '▁この 世界 には 知らない ほう が いい こともある んです よ 。', '▁ まだ 答え は 見つかっ ていない よう です 。 ワ クワ ク します よ ね 。', '▁ あ ー それは 理論 上の 話 です 。', '▁ 宇宙 に 行って みたい です 。', '▁それ もいい か もし れ ません ね 。', '▁ 何 語で 言えば いい のか 分かり ません が 、 伝え て み ます 。', '▁その 可能性 も あります ね 。', '▁ そう です ね 。 2019 年 4 月 10 には 人類 は ブラックホール の撮影 に成功した んです よ 。', '▁ は い 。 是 非 連れ て 行って ください 。', '▁もし 前 澤 さん に 会った ら お 願い して お いて ください 。', '▁ そう ですか 。 目標 を探す の も 楽しい と思います よ 。', '▁ もの す ごく 興味 あります 。 一番 興味 がある と言って も 過 言 では ありません 。', '▁ 夢 は 大きい 方が いい じゃない ですか 。', '▁ そういう 時は 見つかる まで 、 何か できること をして み れば いい か もし れ ません 。', '▁ 宇宙 に興味 がない 人は いない はず です 。', '▁ は い 。 宣言 します 。 そして 実行 します 。', '▁ 分かり ません が 、 が んば る しかない です よ ね 。', '▁ いきなり その 話 ですか 。', '▁ 宇宙 って 解明 されていない ことが たくさん あります よ ね 。 お も しろ い と思います 。', '▁ 言って みたい セリフ です 。', '▁ 怖 くない です 。', '▁ 私の 夢 は 世界中の 人に 会う ことで す 。', '▁ 得意 な ことや 好きな ことは どう かな あ 。', '▁ 何 ですか 。 突然 。', '▁ きっかけ は el on ▁ mus k です 。', '▁ 私の 夢 は 世界の 全ての 国 で働く ことで す 。', '▁ え ー っと 。 誰 にも 言 わない で ください ね 。 私の 夢 は 宇宙 に行く ことで す 。', '▁ 訓練 と 聞く と 大変 そう です ね 。', '▁ いい です よ 。', '▁ いい です ね 。 私 も 世界 一周 して みたい です 。', '▁ え ー 。 それは すごい です ね 。', '▁ そう ですか 。', '▁ 分からない です よ 。 奇跡 が起こる か もし れ ません 。', '▁ 言う の は 簡単な んです けど ね 。', '▁ いい です ね 。', '▁そんな 話を した 覚え はない です ね 。', '▁ そこ はまだ 考え 中 です 。', '▁ 全世界 の 人に 会 って お 話し して みたい です 。', '▁それは 優 雅 です ね 。', '▁ 毎日 勉強 し ています 。 それ ぐらい しか できない の で 。', '▁ いい です ね 。 その 通り です 。', '▁ なる ほど 。', '▁ 一言 では 言 えない です 。', '▁ は い 。 space x は大 好き です 。', '▁ ず ば り 宇宙 に行く ことで す 。', '▁ 宇宙 に行く ことで す 。', '▁ そ り ゃ あ 宇宙 に行き ます よ 。', '▁ お金 さえ あ れば 、 結 構 行 ける ように なってきている らしい です 。', '▁ できる と思います よ 。 el on ▁ mus k が実現 してくれる はず です 。', '▁ は い 。 私の 目標 は 世界の 人 全員 に 会う ことで す 。', '▁ 新しい もの を見る ことが 大 好き です 。', '▁ 夢 ですか 。 そ り ゃ 私 にも あります よ 。', '▁ ビッグ バン によって 始まった と考えられ ています が 、 まだ 正確な ことは 分かって い ません 。', '▁ ロケット 乗 って みたい です 。', '▁ へ ー 。 その 目標 達成 できた ら 教え て ください ね 。', '▁ 勉強 は 嫌い じゃない んです よ 。', '▁そんな 人 いる とは 知り ません で した 。', '▁ いい です けど 、 け っ こう 長い 話 になる か もし れ ません 。', '▁ 一緒に 連れ て いっ てもらえ た らす ごく 嬉 しい です 。', '▁ は い 。 が んば り ます 。', '▁ 急 にその 話 ですか 。', '▁ まだ ない です 。', '▁ へ ー 。 私 と 気 が 合い そう です ね 。']]\n"
     ]
    }
   ],
   "source": [
    "text = SHARED['tokenizer'].parse(\"夢のこと話しましょう\")\n",
    "preds, cand_preds = get_responses(SHARED['agent'], text, \"dream\")\n",
    "print(preds)\n",
    "print(cand_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SHARED['agent'].opt['person_tokens']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "SHARED['agent'].observe({\"text\": \"夢 は 何 です か\", 'episode_done': False, 'topic': \"dream\"})\n",
    "batch = SHARED['agent'].batchify([SHARED['agent'].observation])\n",
    "SHARED['agent'].model.eval()\n",
    "vecs = SHARED['agent'].fixed_candidate_vecs['dream']\n",
    "encs = SHARED['agent'].fixed_candidate_encs['dream']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Batch(text_vec=tensor([[    4,  4764,  4764,     0,  4764,  4764,     9,  1392,  3361,  9016,\n",
       "         24438,     5,  4764,  4764,     0,  4764,  4764,  1392,    11,  1059,\n",
       "             0,    95,     5]], device='cuda:0'), text_lengths=[23], label_vec=None, label_lengths=None, labels=None, valid_indices=[0], candidates=None, candidate_vecs=None, image=None, observations=[{'text': '夢 は 何 です か', 'episode_done': False, 'topic': 'dream', 'full_text': '__p1__ ▁ 夢 のこと 話し ましょう\\n__p1__ 夢 は 何 です か', 'text_vec': tensor([    4,  4764,  4764,     0,  4764,  4764,     9,  1392,  3361,  9016,\n",
       "        24438,     5,  4764,  4764,     0,  4764,  4764,  1392,    11,  1059,\n",
       "            0,    95,     5]), 'added_start_end_tokens': True}], topics=['dream'])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_act = world.batch_act(0, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "world.world.get_agents()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_act"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_obs = world.batch_observe(1, batch_act, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_agent = world.world.get_agents()[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_class = model_agent.dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v =[   1, 1869,  378,    5,    9,    2]\n",
    "for t in v:\n",
    "    print(dict_class.ind2bi[t])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = model_agent.batchify(batch_obs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cands, cand_vecs, label_inds = model_agent._build_candidates(\n",
    "            batch, source='batch', mode='train'\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_inds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cand_vecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch.text_vec.size(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = model_agent.model(batch.text_vec, batch.bi_text_vec, cand_vecs['uni'], cand_vecs['bi'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cand_vecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_uni = batch.text_vec\n",
    "x_bi = batch.bi_text_vec\n",
    "y_uni = cand_vecs['uni']\n",
    "y_bi = cand_vecs['bi']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_emb, y_emb = model_agent.model.encode(x_uni, x_bi, y_uni.unsqueeze(1), y_bi.unsqueeze(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_emb.expand(y_emb.size(0), x_uni.size(0), -1).transpose(0,1).contiguous()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bsz, cand_num, seq_len = y_uni.unsqueeze(1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_emb.view(bsz, cand_num, -1).expand(y_emb.size(0), x_uni.size(0), -1).transpose(0,1).contiguous()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_uni_emb = model_agent.model.y_unigram_encoder(y_uni.unsqueeze(1).view(bsz*cand_num, -1))\n",
    "y_bi_emb = model_agent.model.y_bigram_encoder(y_bi.unsqueeze(1).view(bsz*cand_num, -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_uni_emb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_agent.model.C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_emb.unsqueeze(1).expand(y_emb.view(bsz, cand_num, -1).expand(y_emb.size(0), x_uni.size(0), -1).transpose(0,1).contiguous().shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_emb = y_emb.expand(cand_num, bsz, -1).transpose(0,1).contiguous()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_emb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('/installation/~/ParlAI/data/models/bert_models/bert-wiki-ja/vocab.txt', tokenize_chinese_chars=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[None, None, 8, None, 17, 9444, None, 11, 14045, 57, 40, 57, 8]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.convert_tokens_to_ids(tokenizer.tokenize('そっち かぁ 。 どっち に しろ あめ は ふる な だ な 。'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
