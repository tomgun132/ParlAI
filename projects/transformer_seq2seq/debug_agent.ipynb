{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from parlai.core.params import ParlaiParser\n",
    "from parlai.core.dict import DictionaryAgent\n",
    "from parlai.scripts.train_model import setup_args as tm_setupargs\n",
    "from parlai.scripts.train_model import TrainLoop\n",
    "from parlai.core.agents import create_agent\n",
    "from parlai.core.worlds import create_task\n",
    "from itertools import islice\n",
    "import json\n",
    "import time\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_responses(agent, text, topic):\n",
    "    # topic = classify(text)\n",
    "    obs = {\"text\": text, 'episode_done': False, 'topic': topic}\n",
    "    agent.observe(obs)\n",
    "    batch = agent.batchify([agent.observation])\n",
    "    batchsize = (\n",
    "        batch.text_vec.size(0)\n",
    "        if batch.text_vec is not None\n",
    "        else batch.image.size(0)\n",
    "    )\n",
    "    agent.model.eval()\n",
    "    cands = agent.fixed_candidates[topic]\n",
    "    cand_vecs = agent.fixed_candidate_vecs[topic]\n",
    "    cand_encs = agent.fixed_candidate_encs[topic]\n",
    "    scores = agent.score_candidates(batch, cand_vecs, cand_encs=cand_encs)\n",
    "\n",
    "    if agent.rank_top_k > 0:\n",
    "        sorted_scores, ranks = scores.topk(\n",
    "            min(agent.rank_top_k, scores.size(1)), 1, largest=True\n",
    "        )\n",
    "    else:\n",
    "        sorted_scores, ranks = scores.sort(1, descending=True)\n",
    "\n",
    "    # sorted_scores = F.softmax(sorted_scores, dim=-1)\n",
    "\n",
    "    ranks = ranks.cpu()\n",
    "    max_preds = agent.opt['cap_num_predictions']\n",
    "    cand_preds = []\n",
    "    for i, ordering in enumerate(ranks):\n",
    "        if cand_vecs.dim() == 2:\n",
    "            cand_list = cands\n",
    "        elif cand_vecs.dim() == 3:\n",
    "            cand_list = cands[i]\n",
    "        # using a generator instead of a list comprehension allows\n",
    "        # to cap the number of elements.\n",
    "        cand_preds_generator = (\n",
    "            cand_list[rank] for rank in ordering if rank < len(cand_list)\n",
    "        )\n",
    "        cand_preds.append(list(islice(cand_preds_generator, max_preds)))\n",
    "\n",
    "    if (\n",
    "        agent.opt.get('repeat_blocking_heuristic', True)\n",
    "        and agent.eval_candidates == 'fixed'\n",
    "    ):\n",
    "        cand_preds = agent.block_repeats(cand_preds)\n",
    "\n",
    "    preds = [cand_preds[i][0] for i in range(batchsize)]\n",
    "    return preds, cand_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "SHARED = {}\n",
    "def setup_interactive(shared):\n",
    "    parser = tm_setupargs()\n",
    "    parser.set_params(batchsize=5)\n",
    "    SHARED['opt'] = parser.parse_args([], print_args=True)\n",
    "\n",
    "    SHARED['opt']['task'] = 'wizard_of_wikipedia:generator:random_split'\n",
    "    SHARED['opt']['model'] = 'projects.wizard_of_wikipedia.generator.agents:EndToEndAgent'\n",
    "    SHARED['opt']['dict_tokenizer'] = 'bpe'\n",
    "    SHARED['opt']['dict_lower'] = True\n",
    "    SHARED['opt']['dict_file'] = '/tmp/end2end_generator/model.dict'\n",
    "    SHARED['opt']['model_file'] = '/tmp/end2end_generator/model'\n",
    "    SHARED['opt']['no_cuda'] = True\n",
    "    SHARED['opt']['truncate'] = 128\n",
    "    SHARED['opt']['interactive_mode'] = True\n",
    "    SHARED['opt']['embedding_size'] = 256\n",
    "    SHARED['opt']['n_heads'] = 2\n",
    "    SHARED['opt']['n_layers'] = 5\n",
    "    SHARED['opt']['ffn_size'] = 512\n",
    "    SHARED['opt']['dropout'] = 0.2\n",
    "    SHARED['opt']['n_positions']= 128\n",
    "    SHARED['opt']['max_knowledge']= 32\n",
    "    SHARED['opt']['knowledge_alpha']= 0.95\n",
    "    SHARED['opt']['knowledge_truncate']= 32\n",
    "    SHARED['opt']['learningrate']= 5e-4\n",
    "    SHARED['opt']['warmup_updates']= 5000\n",
    "    SHARED['opt']['clip']= 0.1\n",
    "    SHARED['opt']['lr_scheduler']= 'invsqrt'\n",
    "    SHARED['opt']['embedding_type']= 'fasttext'\n",
    "    SHARED['opt']['beam_size']= 1\n",
    "    SHARED['opt']['skip_generation']= False\n",
    "    \n",
    "    SHARED['opt']['optimizer']= 'adam'\n",
    "    SHARED['opt']['momentum']= 0\n",
    "    SHARED['opt']['warmup_rate']= 1e-4\n",
    "    SHARED['opt']['history_size']= -1\n",
    "    SHARED['opt']['rank_candidates']= False\n",
    "    SHARED['opt']['attention_dropout']= 0.0\n",
    "    SHARED['opt']['relu_dropout']= 0.0\n",
    "    SHARED['opt']['learn_positional_embeddings']= True\n",
    "    SHARED['opt']['embeddings_scale']= True\n",
    "    SHARED['opt']['activation']= 'gelu'\n",
    "    SHARED['opt']['variant']= 'aiayn'\n",
    "    SHARED['opt']['output_scaling']= 1.0\n",
    "    # Create model and assign it to the specified task\n",
    "    SHARED['agent'] = create_agent(SHARED.get('opt'), requireModelExists=False)\n",
    "    SHARED['world'] = create_task(SHARED.get('opt'), [SHARED['agent']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ Main ParlAI Arguments: ] \n",
      "[  batchsize: 5 ]\n",
      "[  datapath: d:\\research\\parlai_seq2seq\\data ]\n",
      "[  datatype: train ]\n",
      "[  download_path: d:\\research\\parlai_seq2seq\\downloads ]\n",
      "[  hide_labels: False ]\n",
      "[  image_mode: raw ]\n",
      "[  init_opt: None ]\n",
      "[  multitask_weights: [1] ]\n",
      "[  numthreads: 1 ]\n",
      "[  show_advanced_args: False ]\n",
      "[  task: None ]\n",
      "[ ParlAI Model Arguments: ] \n",
      "[  dict_class: None ]\n",
      "[  init_model: None ]\n",
      "[  model: None ]\n",
      "[  model_file: None ]\n",
      "[ Training Loop Arguments: ] \n",
      "[  aggregate_micro: False ]\n",
      "[  display_examples: False ]\n",
      "[  eval_batchsize: None ]\n",
      "[  evaltask: None ]\n",
      "[  load_from_checkpoint: False ]\n",
      "[  max_train_time: -1 ]\n",
      "[  metrics: default ]\n",
      "[  num_epochs: -1 ]\n",
      "[  save_after_valid: False ]\n",
      "[  save_every_n_secs: -1 ]\n",
      "[  short_final_eval: False ]\n",
      "[  validation_cutoff: 1.0 ]\n",
      "[  validation_every_n_epochs: -1 ]\n",
      "[  validation_every_n_secs: -1 ]\n",
      "[  validation_max_exs: -1 ]\n",
      "[  validation_metric: accuracy ]\n",
      "[  validation_metric_mode: None ]\n",
      "[  validation_patience: 10 ]\n",
      "[  validation_share_agent: False ]\n",
      "[ Tensorboard Arguments: ] \n",
      "[  tensorboard_log: False ]\n",
      "[ PytorchData Arguments: ] \n",
      "[  batch_length_range: 5 ]\n",
      "[  batch_sort_cache_type: pop ]\n",
      "[  batch_sort_field: text ]\n",
      "[  numworkers: 4 ]\n",
      "[  pytorch_context_length: -1 ]\n",
      "[  pytorch_datapath: None ]\n",
      "[  pytorch_include_labels: True ]\n",
      "[  pytorch_preprocess: False ]\n",
      "[  pytorch_teacher_batch_sort: False ]\n",
      "[  pytorch_teacher_dataset: None ]\n",
      "[  pytorch_teacher_task: None ]\n",
      "[  shuffle: False ]\n",
      "[ Dictionary Loop Arguments: ] \n",
      "[  dict_include_test: False ]\n",
      "[  dict_include_valid: False ]\n",
      "[  dict_maxexs: -1 ]\n",
      "[  log_every_n_secs: 2 ]\n",
      "[ Dictionary Arguments: ] \n",
      "[  bpe_debug: False ]\n",
      "[  dict_endtoken: __end__ ]\n",
      "[  dict_file: None ]\n",
      "[  dict_initpath: None ]\n",
      "[  dict_language: english ]\n",
      "[  dict_lower: False ]\n",
      "[  dict_max_ngram_size: -1 ]\n",
      "[  dict_maxtokens: -1 ]\n",
      "[  dict_minfreq: 0 ]\n",
      "[  dict_nulltoken: __null__ ]\n",
      "[  dict_starttoken: __start__ ]\n",
      "[  dict_textfields: text,labels ]\n",
      "[  dict_tokenizer: re ]\n",
      "[  dict_unktoken: __unk__ ]\n",
      "[ ParlAI Image Preprocessing Arguments: ] \n",
      "[  image_cropsize: 224 ]\n",
      "[  image_size: 256 ]\n",
      "[ Current ParlAI commit: 8abb98809f378076d689098b69685b1d34e57535 ]\n",
      "[ no model with opt yet at: /tmp/end2end_generator/model(.opt) ]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\research\\parlai_seq2seq\\parlai\\agents\\transformer\\modules.py:33: UserWarning: Installing APEX can give a significant speed boost.\n",
      "  warn_once(\"Installing APEX can give a significant speed boost.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[EndToEnd: full interactive mode on.]\n",
      "Initialized embeddings for 1 tokens (25.0%) from fasttext.\n",
      "Total parameters: 6656000\n",
      "Trainable parameters:  6656000\n",
      "[creating task(s): wizard_of_wikipedia:generator:random_split]\n",
      "loading: d:\\research\\parlai_seq2seq\\data\\wizard_of_wikipedia\\train.json\n"
     ]
    }
   ],
   "source": [
    "setup_interactive(SHARED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 'wizard_of_wikipedia',\n",
       " 'opt': {'init_opt': None,\n",
       "  'show_advanced_args': False,\n",
       "  'task': 'wizard_of_wikipedia',\n",
       "  'download_path': 'd:\\\\research\\\\parlai_seq2seq\\\\downloads',\n",
       "  'datatype': 'train',\n",
       "  'image_mode': 'raw',\n",
       "  'numthreads': 1,\n",
       "  'hide_labels': False,\n",
       "  'multitask_weights': [1],\n",
       "  'batchsize': 5,\n",
       "  'datapath': 'd:\\\\research\\\\parlai_seq2seq\\\\data',\n",
       "  'model': 'projects.wizard_of_wikipedia.generator.agents:EndToEndAgent',\n",
       "  'model_file': '/tmp/end2end_generator/model',\n",
       "  'init_model': None,\n",
       "  'dict_class': None,\n",
       "  'evaltask': None,\n",
       "  'eval_batchsize': None,\n",
       "  'display_examples': False,\n",
       "  'num_epochs': -1,\n",
       "  'max_train_time': -1,\n",
       "  'validation_every_n_secs': -1,\n",
       "  'save_every_n_secs': -1,\n",
       "  'save_after_valid': False,\n",
       "  'validation_every_n_epochs': -1,\n",
       "  'validation_max_exs': -1,\n",
       "  'short_final_eval': False,\n",
       "  'validation_patience': 10,\n",
       "  'validation_metric': 'accuracy',\n",
       "  'validation_metric_mode': None,\n",
       "  'validation_cutoff': 1.0,\n",
       "  'load_from_checkpoint': False,\n",
       "  'validation_share_agent': False,\n",
       "  'aggregate_micro': False,\n",
       "  'metrics': 'default',\n",
       "  'tensorboard_log': False,\n",
       "  'pytorch_teacher_task': None,\n",
       "  'pytorch_teacher_dataset': None,\n",
       "  'pytorch_datapath': None,\n",
       "  'numworkers': 4,\n",
       "  'pytorch_preprocess': False,\n",
       "  'pytorch_teacher_batch_sort': False,\n",
       "  'batch_sort_cache_type': 'pop',\n",
       "  'batch_length_range': 5,\n",
       "  'shuffle': False,\n",
       "  'batch_sort_field': 'text',\n",
       "  'pytorch_context_length': -1,\n",
       "  'pytorch_include_labels': True,\n",
       "  'dict_maxexs': -1,\n",
       "  'dict_include_valid': False,\n",
       "  'dict_include_test': False,\n",
       "  'log_every_n_secs': 2,\n",
       "  'dict_file': '/tmp/end2end_generator/model.dict',\n",
       "  'dict_initpath': None,\n",
       "  'dict_language': 'english',\n",
       "  'dict_max_ngram_size': -1,\n",
       "  'dict_minfreq': 0,\n",
       "  'dict_maxtokens': -1,\n",
       "  'dict_nulltoken': '__null__',\n",
       "  'dict_starttoken': '__start__',\n",
       "  'dict_endtoken': '__end__',\n",
       "  'dict_unktoken': '__unk__',\n",
       "  'dict_tokenizer': 'bpe',\n",
       "  'dict_lower': True,\n",
       "  'bpe_debug': False,\n",
       "  'dict_textfields': 'text,labels',\n",
       "  'image_size': 256,\n",
       "  'image_cropsize': 224,\n",
       "  'parlai_home': 'd:\\\\research\\\\parlai_seq2seq',\n",
       "  'override': {'batchsize': 5},\n",
       "  'starttime': 'Jan14_12-56',\n",
       "  'no_cuda': True,\n",
       "  'truncate': 128,\n",
       "  'interactive_mode': True,\n",
       "  'embedding_size': 256,\n",
       "  'n_heads': 2,\n",
       "  'n_layers': 5,\n",
       "  'ffn_size': 512,\n",
       "  'dropout': 0.2,\n",
       "  'n_positions': 128,\n",
       "  'max_knowledge': 32,\n",
       "  'knowledge_alpha': 0.95,\n",
       "  'knowledge_truncate': 32,\n",
       "  'learningrate': 0.0005,\n",
       "  'warmup_updates': 5000,\n",
       "  'clip': 0.1,\n",
       "  'lr_scheduler': 'invsqrt',\n",
       "  'embedding_type': 'fasttext',\n",
       "  'beam_size': 1,\n",
       "  'skip_generation': False,\n",
       "  'optimizer': 'adam',\n",
       "  'momentum': 0,\n",
       "  'warmup_rate': 0.0001,\n",
       "  'history_size': -1,\n",
       "  'rank_candidates': False,\n",
       "  'attention_dropout': 0.0,\n",
       "  'relu_dropout': 0.0,\n",
       "  'learn_positional_embeddings': True,\n",
       "  'embeddings_scale': True,\n",
       "  'activation': 'gelu',\n",
       "  'variant': 'aiayn',\n",
       "  'output_scaling': 1.0,\n",
       "  'label_type': 'response',\n",
       "  'include_checked_sentence': True,\n",
       "  'batchindex': 4},\n",
       " 'agents': None,\n",
       " 'max_exs': None,\n",
       " 'total_exs': 0,\n",
       " 'total_epochs': 0,\n",
       " 'total_parleys': 0,\n",
       " 'time': <parlai.utils.misc.Timer at 0x1c843920188>,\n",
       " 'random': True,\n",
       " 'world': <parlai.core.worlds.DialogPartnerWorld at 0x1c78c8fac08>,\n",
       " 'worlds': [<parlai.core.worlds.DialogPartnerWorld at 0x1c78ab88ec8>,\n",
       "  <parlai.core.worlds.DialogPartnerWorld at 0x1c843920f08>,\n",
       "  <parlai.core.worlds.DialogPartnerWorld at 0x1c84391d3c8>,\n",
       "  <parlai.core.worlds.DialogPartnerWorld at 0x1c843929308>,\n",
       "  <parlai.core.worlds.DialogPartnerWorld at 0x1c84392c948>],\n",
       " 'batch_observations': [None, None],\n",
       " 'first_batch': None,\n",
       " 'acts': [None, None]}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SHARED['world'].__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 'WizTeacher',\n",
       " 'opt': {'init_opt': None,\n",
       "  'show_advanced_args': False,\n",
       "  'task': 'wizard_of_wikipedia',\n",
       "  'download_path': 'd:\\\\research\\\\parlai_seq2seq\\\\downloads',\n",
       "  'datatype': 'train',\n",
       "  'image_mode': 'raw',\n",
       "  'numthreads': 1,\n",
       "  'hide_labels': False,\n",
       "  'multitask_weights': [1],\n",
       "  'batchsize': 5,\n",
       "  'datapath': 'd:\\\\research\\\\parlai_seq2seq\\\\data',\n",
       "  'model': 'projects.wizard_of_wikipedia.generator.agents:EndToEndAgent',\n",
       "  'model_file': '/tmp/end2end_generator/model',\n",
       "  'init_model': None,\n",
       "  'dict_class': None,\n",
       "  'evaltask': None,\n",
       "  'eval_batchsize': None,\n",
       "  'display_examples': False,\n",
       "  'num_epochs': -1,\n",
       "  'max_train_time': -1,\n",
       "  'validation_every_n_secs': -1,\n",
       "  'save_every_n_secs': -1,\n",
       "  'save_after_valid': False,\n",
       "  'validation_every_n_epochs': -1,\n",
       "  'validation_max_exs': -1,\n",
       "  'short_final_eval': False,\n",
       "  'validation_patience': 10,\n",
       "  'validation_metric': 'accuracy',\n",
       "  'validation_metric_mode': None,\n",
       "  'validation_cutoff': 1.0,\n",
       "  'load_from_checkpoint': False,\n",
       "  'validation_share_agent': False,\n",
       "  'aggregate_micro': False,\n",
       "  'metrics': 'default',\n",
       "  'tensorboard_log': False,\n",
       "  'pytorch_teacher_task': None,\n",
       "  'pytorch_teacher_dataset': None,\n",
       "  'pytorch_datapath': None,\n",
       "  'numworkers': 4,\n",
       "  'pytorch_preprocess': False,\n",
       "  'pytorch_teacher_batch_sort': False,\n",
       "  'batch_sort_cache_type': 'pop',\n",
       "  'batch_length_range': 5,\n",
       "  'shuffle': False,\n",
       "  'batch_sort_field': 'text',\n",
       "  'pytorch_context_length': -1,\n",
       "  'pytorch_include_labels': True,\n",
       "  'dict_maxexs': -1,\n",
       "  'dict_include_valid': False,\n",
       "  'dict_include_test': False,\n",
       "  'log_every_n_secs': 2,\n",
       "  'dict_file': '/tmp/end2end_generator/model.dict',\n",
       "  'dict_initpath': None,\n",
       "  'dict_language': 'english',\n",
       "  'dict_max_ngram_size': -1,\n",
       "  'dict_minfreq': 0,\n",
       "  'dict_maxtokens': -1,\n",
       "  'dict_nulltoken': '__null__',\n",
       "  'dict_starttoken': '__start__',\n",
       "  'dict_endtoken': '__end__',\n",
       "  'dict_unktoken': '__unk__',\n",
       "  'dict_tokenizer': 'bpe',\n",
       "  'dict_lower': True,\n",
       "  'bpe_debug': False,\n",
       "  'dict_textfields': 'text,labels',\n",
       "  'image_size': 256,\n",
       "  'image_cropsize': 224,\n",
       "  'parlai_home': 'd:\\\\research\\\\parlai_seq2seq',\n",
       "  'override': {'batchsize': 5},\n",
       "  'starttime': 'Jan14_12-56',\n",
       "  'no_cuda': True,\n",
       "  'truncate': 128,\n",
       "  'interactive_mode': True,\n",
       "  'embedding_size': 256,\n",
       "  'n_heads': 2,\n",
       "  'n_layers': 5,\n",
       "  'ffn_size': 512,\n",
       "  'dropout': 0.2,\n",
       "  'n_positions': 128,\n",
       "  'max_knowledge': 32,\n",
       "  'knowledge_alpha': 0.95,\n",
       "  'knowledge_truncate': 32,\n",
       "  'learningrate': 0.0005,\n",
       "  'warmup_updates': 5000,\n",
       "  'clip': 0.1,\n",
       "  'lr_scheduler': 'invsqrt',\n",
       "  'embedding_type': 'fasttext',\n",
       "  'beam_size': 1,\n",
       "  'skip_generation': False,\n",
       "  'optimizer': 'adam',\n",
       "  'momentum': 0,\n",
       "  'warmup_rate': 0.0001,\n",
       "  'history_size': -1,\n",
       "  'rank_candidates': False,\n",
       "  'attention_dropout': 0.0,\n",
       "  'relu_dropout': 0.0,\n",
       "  'learn_positional_embeddings': True,\n",
       "  'embeddings_scale': True,\n",
       "  'activation': 'gelu',\n",
       "  'variant': 'aiayn',\n",
       "  'output_scaling': 1.0,\n",
       "  'label_type': 'response',\n",
       "  'include_checked_sentence': True,\n",
       "  'batchindex': 0},\n",
       " 'agents': [<parlai.tasks.wizard_of_wikipedia.agents.GeneratorTeacher at 0x1c843920fc8>,\n",
       "  <projects.wizard_of_wikipedia.generator.agents.EndToEndAgent at 0x1c84391de48>],\n",
       " 'max_exs': None,\n",
       " 'total_exs': 0,\n",
       " 'total_epochs': 0,\n",
       " 'total_parleys': 0,\n",
       " 'time': <parlai.utils.misc.Timer at 0x1c843920cc8>,\n",
       " 'acts': [None, None]}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SHARED['world'].worlds[0].__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'opt': {'init_opt': None,\n",
       "  'show_advanced_args': False,\n",
       "  'task': 'wizard_of_wikipedia:generator:random_split',\n",
       "  'download_path': 'd:\\\\research\\\\parlai_seq2seq\\\\downloads',\n",
       "  'datatype': 'train',\n",
       "  'image_mode': 'raw',\n",
       "  'numthreads': 1,\n",
       "  'hide_labels': False,\n",
       "  'multitask_weights': [1],\n",
       "  'batchsize': 5,\n",
       "  'datapath': 'd:\\\\research\\\\parlai_seq2seq\\\\data',\n",
       "  'model': 'projects.wizard_of_wikipedia.generator.agents:EndToEndAgent',\n",
       "  'model_file': '/tmp/end2end_generator/model',\n",
       "  'init_model': None,\n",
       "  'dict_class': None,\n",
       "  'evaltask': None,\n",
       "  'eval_batchsize': None,\n",
       "  'display_examples': False,\n",
       "  'num_epochs': -1,\n",
       "  'max_train_time': -1,\n",
       "  'validation_every_n_secs': -1,\n",
       "  'save_every_n_secs': -1,\n",
       "  'save_after_valid': False,\n",
       "  'validation_every_n_epochs': -1,\n",
       "  'validation_max_exs': -1,\n",
       "  'short_final_eval': False,\n",
       "  'validation_patience': 10,\n",
       "  'validation_metric': 'accuracy',\n",
       "  'validation_metric_mode': None,\n",
       "  'validation_cutoff': 1.0,\n",
       "  'load_from_checkpoint': False,\n",
       "  'validation_share_agent': False,\n",
       "  'aggregate_micro': False,\n",
       "  'metrics': 'default',\n",
       "  'tensorboard_log': False,\n",
       "  'pytorch_teacher_task': None,\n",
       "  'pytorch_teacher_dataset': None,\n",
       "  'pytorch_datapath': None,\n",
       "  'numworkers': 4,\n",
       "  'pytorch_preprocess': False,\n",
       "  'pytorch_teacher_batch_sort': False,\n",
       "  'batch_sort_cache_type': 'pop',\n",
       "  'batch_length_range': 5,\n",
       "  'shuffle': False,\n",
       "  'batch_sort_field': 'text',\n",
       "  'pytorch_context_length': -1,\n",
       "  'pytorch_include_labels': True,\n",
       "  'dict_maxexs': -1,\n",
       "  'dict_include_valid': False,\n",
       "  'dict_include_test': False,\n",
       "  'log_every_n_secs': 2,\n",
       "  'dict_file': '/tmp/end2end_generator/model.dict',\n",
       "  'dict_initpath': None,\n",
       "  'dict_language': 'english',\n",
       "  'dict_max_ngram_size': -1,\n",
       "  'dict_minfreq': 0,\n",
       "  'dict_maxtokens': -1,\n",
       "  'dict_nulltoken': '__null__',\n",
       "  'dict_starttoken': '__start__',\n",
       "  'dict_endtoken': '__end__',\n",
       "  'dict_unktoken': '__unk__',\n",
       "  'dict_tokenizer': 'bpe',\n",
       "  'dict_lower': True,\n",
       "  'bpe_debug': False,\n",
       "  'dict_textfields': 'text,labels',\n",
       "  'image_size': 256,\n",
       "  'image_cropsize': 224,\n",
       "  'parlai_home': 'd:\\\\research\\\\parlai_seq2seq',\n",
       "  'override': {'batchsize': 5},\n",
       "  'starttime': 'Jan14_12-56',\n",
       "  'no_cuda': True,\n",
       "  'truncate': 128,\n",
       "  'interactive_mode': True,\n",
       "  'embedding_size': 256,\n",
       "  'n_heads': 2,\n",
       "  'n_layers': 5,\n",
       "  'ffn_size': 512,\n",
       "  'dropout': 0.2,\n",
       "  'n_positions': 128,\n",
       "  'max_knowledge': 32,\n",
       "  'knowledge_alpha': 0.95,\n",
       "  'knowledge_truncate': 32,\n",
       "  'learningrate': 0.0005,\n",
       "  'warmup_updates': 5000,\n",
       "  'clip': 0.1,\n",
       "  'lr_scheduler': 'invsqrt',\n",
       "  'embedding_type': 'fasttext',\n",
       "  'beam_size': 1,\n",
       "  'skip_generation': False,\n",
       "  'optimizer': 'adam',\n",
       "  'momentum': 0,\n",
       "  'warmup_rate': 0.0001,\n",
       "  'history_size': -1,\n",
       "  'rank_candidates': False,\n",
       "  'attention_dropout': 0.0,\n",
       "  'relu_dropout': 0.0,\n",
       "  'learn_positional_embeddings': True,\n",
       "  'embeddings_scale': True,\n",
       "  'activation': 'gelu',\n",
       "  'variant': 'aiayn',\n",
       "  'output_scaling': 1.0},\n",
       " 'minfreq': 0,\n",
       " 'null_token': '__null__',\n",
       " 'end_token': '__end__',\n",
       " 'unk_token': '__unk__',\n",
       " 'start_token': '__start__',\n",
       " 'max_ngram_size': -1,\n",
       " 'tokenizer': 'bpe',\n",
       " 'lower': True,\n",
       " 'maxtokens': -1,\n",
       " 'textfields': ['text', 'labels'],\n",
       " 'tokenizer_fun': <bound method DictionaryAgent.bpe_tokenize of <parlai.core.dict.DictionaryAgent object at 0x000001C78ABA3888>>,\n",
       " 'freq': defaultdict(int,\n",
       "             {'__null__': 1000000003,\n",
       "              '__start__': 1000000002,\n",
       "              '__end__': 1000000001,\n",
       "              '__unk__': 1000000000}),\n",
       " 'tok2ind': {'__null__': 0, '__start__': 1, '__end__': 2, '__unk__': 3},\n",
       " 'ind2tok': {0: '__null__', 1: '__start__', 2: '__end__', 3: '__unk__'},\n",
       " 'bpehelper': <parlai.core.dict._BPEHelper at 0x1c78ad64748>,\n",
       " 'save_path': '/tmp/end2end_generator/model.dict'}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SHARED['agent'].dict.__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_act = SHARED['world'].batch_act(0, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 'WizTeacher',\n",
       " 'text': 'Snowboarding',\n",
       " 'labels': ['I love to go snowboarding on the ski slopes in the winter'],\n",
       " 'chosen_topic': 'Snowboarding',\n",
       " 'episode_done': False,\n",
       " 'label_candidates': [],\n",
       " 'knowledge': 'no_passages_used __knowledge__ no_passages_used\\nSnowboarding __knowledge__ Snowboarding is a recreational activity and Olympic and Paralympic sport that involves descending a snow-covered slope while standing on a snowboard attached to a rider\\'s feet.\\nSnowboarding __knowledge__ The development of snowboarding was inspired by skateboarding, sledding, surfing and skiing.\\nSnowboarding __knowledge__ It was developed in the United States in the 1960s, became a Winter Olympic Sport at Nagano in 1998 and first featured in the Winter Paralympics at Sochi in 2014.\\nSnowboarding __knowledge__ Its popularity (as measured by equipment sales) in the United States peaked in 2007 and has been in a decline since.\\nSnowboarding __knowledge__ Modern snowboarding began in 1965 when Sherman Poppen, an engineer in Muskegon, Michigan, invented a toy for his daughters by fastening two skis together and attaching a rope to one end so he would have some control as they stood on the board and glided downhill.\\nSnowboarding __knowledge__ Dubbed the \"snurfer\" (combining snow and surfer) by his wife Nancy, the toy proved so popular among his daughters\\' friends that Poppen licensed the idea to a manufacturer, Brunswick Corporation, that sold about a million snurfers over the next decade.\\nSnowboarding __knowledge__ And, in 1966 alone over half a million snurfers were sold.\\nSnowboarding __knowledge__ In February 1968, Poppen organized the first snurfing competition at a Michigan ski resort that attracted enthusiasts from all over the country.\\n',\n",
       " 'title': 'Snowboarding',\n",
       " 'checked_sentence': \"Snowboarding is a recreational activity and Olympic and Paralympic sport that involves descending a snow-covered slope while standing on a snowboard attached to a rider's feet.\"}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_act[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "SHARED['agent'].observe({\"text\": \"夢 は 何 です か\", 'episode_done': False, 'topic': \"dream\"})\n",
    "batch = SHARED['agent'].batchify([SHARED['agent'].observation])\n",
    "SHARED['agent'].model.eval()\n",
    "vecs = SHARED['agent'].fixed_candidate_vecs['dream']\n",
    "encs = SHARED['agent'].fixed_candidate_encs['dream']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_obs = SHARED['world'].batch_observe(1, batch_act, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 'WizTeacher',\n",
       " 'text': 'Snowboarding',\n",
       " 'labels': ['I love to go snowboarding on the ski slopes in the winter'],\n",
       " 'chosen_topic': 'Snowboarding',\n",
       " 'episode_done': False,\n",
       " 'label_candidates': [],\n",
       " 'knowledge': 'no_passages_used __knowledge__ no_passages_used\\nSnowboarding __knowledge__ Snowboarding is a recreational activity and Olympic and Paralympic sport that involves descending a snow-covered slope while standing on a snowboard attached to a rider\\'s feet.\\nSnowboarding __knowledge__ The development of snowboarding was inspired by skateboarding, sledding, surfing and skiing.\\nSnowboarding __knowledge__ It was developed in the United States in the 1960s, became a Winter Olympic Sport at Nagano in 1998 and first featured in the Winter Paralympics at Sochi in 2014.\\nSnowboarding __knowledge__ Its popularity (as measured by equipment sales) in the United States peaked in 2007 and has been in a decline since.\\nSnowboarding __knowledge__ Modern snowboarding began in 1965 when Sherman Poppen, an engineer in Muskegon, Michigan, invented a toy for his daughters by fastening two skis together and attaching a rope to one end so he would have some control as they stood on the board and glided downhill.\\nSnowboarding __knowledge__ Dubbed the \"snurfer\" (combining snow and surfer) by his wife Nancy, the toy proved so popular among his daughters\\' friends that Poppen licensed the idea to a manufacturer, Brunswick Corporation, that sold about a million snurfers over the next decade.\\nSnowboarding __knowledge__ And, in 1966 alone over half a million snurfers were sold.\\nSnowboarding __knowledge__ In February 1968, Poppen organized the first snurfing competition at a Michigan ski resort that attracted enthusiasts from all over the country.\\n',\n",
       " 'title': 'Snowboarding',\n",
       " 'checked_sentence': \"Snowboarding is a recreational activity and Olympic and Paralympic sport that involves descending a snow-covered slope while standing on a snowboard attached to a rider's feet.\",\n",
       " 'full_text': 'Snowboarding',\n",
       " 'text_vec': tensor([3]),\n",
       " 'labels_vec': tensor([3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 2]),\n",
       " 'labels_choice': 'I love to go snowboarding on the ski slopes in the winter'}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_obs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_agent = world.world.get_agents()[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_class = model_agent.dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v =[   1, 1869,  378,    5,    9,    2]\n",
    "for t in v:\n",
    "    print(dict_class.ind2bi[t])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = model_agent.batchify(batch_obs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cands, cand_vecs, label_inds = model_agent._build_candidates(\n",
    "            batch, source='batch', mode='train'\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_inds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cand_vecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch.text_vec.size(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = model_agent.model(batch.text_vec, batch.bi_text_vec, cand_vecs['uni'], cand_vecs['bi'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cand_vecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_uni = batch.text_vec\n",
    "x_bi = batch.bi_text_vec\n",
    "y_uni = cand_vecs['uni']\n",
    "y_bi = cand_vecs['bi']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_emb, y_emb = model_agent.model.encode(x_uni, x_bi, y_uni.unsqueeze(1), y_bi.unsqueeze(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_emb.expand(y_emb.size(0), x_uni.size(0), -1).transpose(0,1).contiguous()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bsz, cand_num, seq_len = y_uni.unsqueeze(1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_emb.view(bsz, cand_num, -1).expand(y_emb.size(0), x_uni.size(0), -1).transpose(0,1).contiguous()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_uni_emb = model_agent.model.y_unigram_encoder(y_uni.unsqueeze(1).view(bsz*cand_num, -1))\n",
    "y_bi_emb = model_agent.model.y_bigram_encoder(y_bi.unsqueeze(1).view(bsz*cand_num, -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_uni_emb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_agent.model.C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_emb.unsqueeze(1).expand(y_emb.view(bsz, cand_num, -1).expand(y_emb.size(0), x_uni.size(0), -1).transpose(0,1).contiguous().shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_emb = y_emb.expand(cand_num, bsz, -1).transpose(0,1).contiguous()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_emb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('/installation/~/ParlAI/data/models/bert_models/bert-wiki-ja/vocab.txt', tokenize_chinese_chars=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[None, None, 8, None, 17, 9444, None, 11, 14045, 57, 40, 57, 8]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.convert_tokens_to_ids(tokenizer.tokenize('そっち かぁ 。 どっち に しろ あめ は ふる な だ な 。'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
